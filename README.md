# ü§ñ J.A.R.V.I.S - Just A Rather Very Intelligent System

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![Ollama](https://img.shields.io/badge/Powered%20by-Ollama-green.svg)](https://ollama.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> *Un assistant IA personnel avanc√© inspir√© de l'assistant de Tony Stark, capable de raisonnement complexe, d'actions autonomes et d'apprentissage continu.*

## üìã Table des Mati√®res

- [√Ä Propos](#-√†-propos)
- [Caract√©ristiques](#-caract√©ristiques)
- [Architecture](#-architecture)
- [Pr√©requis](#-pr√©requis)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Utilisation](#-utilisation)
- [Actions Disponibles](#-actions-disponibles)
- [Structure du Projet](#-structure-du-projet)
- [Fonctionnement du Syst√®me Agentique](#-fonctionnement-du-syst√®me-agentique)
- [Personnalisation](#-personnalisation)
- [D√©pannage](#-d√©pannage)
- [Contribuer](#-contribuer)
- [Licence](#-licence)

## üéØ √Ä Propos

**J.A.R.V.I.S** est un assistant IA avanc√© qui fonctionne enti√®rement en local gr√¢ce √† **Ollama**. Inspir√© de l'assistant personnel de Tony Stark, ce syst√®me impl√©mente une architecture agentique compl√®te avec:

- üß† **Raisonnement multi-√©tapes** : Utilise une boucle agentique (ReAct pattern) pour des t√¢ches complexes
- üîß **Actions autonomes** : Peut interagir avec votre syst√®me, le web, et les fichiers
- üí≠ **Pens√©e visible** : Affiche son processus de r√©flexion interne
- üìù **M√©moire conversationnelle** : Maintient le contexte de vos √©changes
- üé® **Interface √©l√©gante** : Terminal riche avec formatage Markdown et couleurs

Le syst√®me suit un **pipeline cognitif** sophistiqu√© pour chaque requ√™te utilisateur, analysant l'intention, planifiant les √©tapes, et ex√©cutant les actions n√©cessaires de mani√®re autonome.

## ‚ú® Caract√©ristiques

### ü§ñ Intelligence Avanc√©e

- **Mod√®le LLM Local** : Utilise Llama 3.1 8B via Ollama (personnalisable)
- **Raisonnement Complexe** : Jusqu'√† 25 √©tapes de r√©flexion pour r√©soudre des probl√®mes complexes
- **Pattern ReAct** : Observation ‚Üí Pens√©e ‚Üí Action ‚Üí Observation (boucle it√©rative)
- **Verbosit√© √âlev√©e** : R√©pond avec des explications d√©taill√©es de type essai universitaire

### üõ†Ô∏è Actions Multiples

Le syst√®me dispose de 14+ actions r√©parties en 5 cat√©gories:

- **Web** : Recherche Wikipedia, YouTube, Google
- **Syst√®me de Fichiers** : Lecture, √©criture, listage de r√©pertoires
- **Syst√®me** : Ouverture d'applications
- **Internet** : Recherche web DuckDuckGo, extraction de contenu web
- **Extensions** : Statistiques syst√®me, captures d'√©cran

### üé® Interface Utilisateur

- **Terminal Enrichi** : Utilise la biblioth√®que `rich` pour une interface moderne
- **Visualisation des Pens√©es** : Affiche le processus de r√©flexion de l'IA
- **Indicateurs d'Actions** : Notifications visuelles lors de l'ex√©cution d'actions
- **Support Markdown** : Rendu √©l√©gant des r√©ponses format√©es
- **Th√®me Personnalis√©** : Couleurs inspir√©es de l'univers Iron Man (cyan/or)

### üìä Rapports Automatiques

Chaque conversation est automatiquement sauvegard√©e dans le dossier `reports/` avec:
- Horodatage
- Extrait de la requ√™te utilisateur
- Contenu complet de la r√©ponse
- Format Markdown

## üèóÔ∏è Architecture

Le projet suit une architecture modulaire claire:

```
jarvis/
‚îú‚îÄ‚îÄ main.py              # Point d'entr√©e et boucle agentique
‚îú‚îÄ‚îÄ config.py            # Configuration (mod√®le, API keys)
‚îú‚îÄ‚îÄ ui.py                # Interface utilisateur (Rich)
‚îú‚îÄ‚îÄ utils.py             # Utilitaires (parsing JSON)
‚îú‚îÄ‚îÄ system_prompt.txt    # Prompt syst√®me d√©taill√©
‚îú‚îÄ‚îÄ requirements.txt     # D√©pendances Python
‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îî‚îÄ‚îÄ engine.py        # Moteur LLM (Ollama)
‚îú‚îÄ‚îÄ actions/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py      # Registre des outils
‚îÇ   ‚îú‚îÄ‚îÄ web.py           # Actions web
‚îÇ   ‚îú‚îÄ‚îÄ filesystem.py    # Actions fichiers
‚îÇ   ‚îú‚îÄ‚îÄ system.py        # Actions syst√®me
‚îÇ   ‚îú‚îÄ‚îÄ internet.py      # Actions internet
‚îÇ   ‚îî‚îÄ‚îÄ extended.py      # Actions √©tendues
‚îî‚îÄ‚îÄ reports/             # Rapports de conversation
```

### üîÑ Flux d'Ex√©cution

```mermaid
graph TD
    A[Utilisateur] -->|Requ√™te| B[Main Loop]
    B --> C[LLM Generate]
    C --> D{Type?}
    D -->|chat| E[Afficher R√©ponse]
    D -->|action| F[Ex√©cuter Action]
    F --> G[R√©sultat]
    G --> H[Ajouter Observation]
    H --> C
    E --> I[Sauvegarder Rapport]
    I --> A
```

## üîß Pr√©requis

### Logiciels Requis

- **Python 3.8+** : [T√©l√©charger Python](https://www.python.org/downloads/)
- **Ollama** : [Installer Ollama](https://ollama.com/)
- **Git** (optionnel) : Pour cloner le d√©p√¥t

### Syst√®me d'Exploitation

- ‚úÖ Windows 10/11
- ‚úÖ macOS (10.15+)
- ‚úÖ Linux (Ubuntu 20.04+, Debian, Fedora)

### Mat√©riel Recommand√©

- **RAM** : 8 Go minimum (16 Go recommand√©)
- **Stockage** : 10 Go d'espace libre (pour les mod√®les)
- **Processeur** : CPU moderne multi-c≈ìur
- **GPU** (optionnel) : NVIDIA pour acc√©l√©ration CUDA

## üì¶ Installation

### 1. Cloner le D√©p√¥t

```bash
git clone <URL_DU_DEPOT>
cd jarvis
```

### 2. Cr√©er un Environnement Virtuel

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/macOS
python3 -m venv venv
source venv/bin/activate
```

### 3. Installer les D√©pendances

```bash
pip install -r requirements.txt
```

### 4. Installer Ollama

Si ce n'est pas d√©j√† fait:

**Windows:**
```bash
# T√©l√©charger depuis https://ollama.com/download/windows
# Ou via winget:
winget install Ollama.Ollama
```

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 5. T√©l√©charger le Mod√®le LLM

```bash
ollama pull llama3.1:8b
```

*Note: Le syst√®me t√©l√©chargera automatiquement le mod√®le au premier lancement si absent.*

## ‚öôÔ∏è Configuration

### Fichier `config.py`

Personnalisez le comportement de J.A.R.V.I.S:

```python
# Mod√®le LLM (voir 'ollama list' pour les mod√®les disponibles)
MODEL_ID = "llama3.1:8b"  # Options: llama3.1, codellama, mistral, etc.

# URL Ollama
OLLAMA_URL = "http://localhost:11434"

# Cl√©s API (optionnelles)
GOOGLE_API_KEY = "votre_cl√©_api"      # Pour google_search()
GOOGLE_CSE_ID = "votre_cse_id"        # Pour google_search()
```

### Personnalisation du Prompt Syst√®me

√âditez `system_prompt.txt` pour modifier:
- La personnalit√© de l'assistant
- Le niveau de verbosit√©
- Les directives de comportement
- Le pipeline cognitif

## üöÄ Utilisation

### Lancement Standard

```bash
python main.py
```

### Mode Mock (Sans LLM)

Pour tester sans charger le mod√®le:

```bash
python main.py --mock
```

### Lancement via Script Batch (Windows)

Double-cliquez sur `run.bat` ou:

```bash
run.bat
```

### Exemples de Requ√™tes

**Recherche d'information:**
```
User: Explique-moi la th√©orie de la relativit√©
```

**Action syst√®me:**
```
User: Quelle est l'utilisation actuelle du CPU et de la RAM ?
```

**Recherche web:**
```
User: Trouve-moi des informations sur les derni√®res avanc√©es en IA
```

**Manipulation de fichiers:**
```
User: Liste les fichiers du dossier Documents
```

**Multi-√©tapes:**
```
User: Recherche des informations sur Python, puis cr√©e un fichier r√©sum√©.txt avec ce que tu as trouv√©
```

### Commandes Sp√©ciales

- `exit` / `quit` / `q` / `quitter` : Quitter l'assistant

## üéØ Actions Disponibles

### üåê Actions Web (`actions/web.py`)

| Action | Description | Arguments |
|--------|-------------|-----------|
| `wikipedia_search` | Recherche sur Wikipedia (FR) | `query` (str) |
| `youtube_search` | Ouvre une recherche YouTube | `query` (str) |
| `google_search` | Recherche Google via API ou navigateur | `query` (str) |

### üìÅ Actions Syst√®me de Fichiers (`actions/filesystem.py`)

| Action | Description | Arguments |
|--------|-------------|-----------|
| `read_file` | Lit le contenu d'un fichier | `path` (str) |
| `write_file` | √âcrit dans un fichier | `path`, `content` |
| `list_dir` | Liste les fichiers d'un r√©pertoire | `path` (str, d√©faut: ".") |

### üíª Actions Syst√®me (`actions/system.py`)

| Action | Description | Arguments |
|--------|-------------|-----------|
| `open_app` | Ouvre une application ou un fichier | `app_name` (str) |

### üåç Actions Internet (`actions/internet.py`)

| Action | Description | Arguments |
|--------|-------------|-----------|
| `search_web` | Recherche DuckDuckGo (top 5) | `query` (str) |
| `visit_page` | Extrait le texte d'une page web | `url` (str) |

### üîß Actions √âtendues (`actions/extended.py`)

| Action | Description | Arguments |
|--------|-------------|-----------|
| `system_stats` | Statistiques CPU/RAM | Aucun |
| `take_screenshot` | Capture d'√©cran | `filename` (optionnel) |

## üìÇ Structure du Projet

### Fichiers Principaux

- **`main.py`** : Boucle principale, gestion de l'agent, sauvegarde des rapports
- **`config.py`** : Configuration centralis√©e (mod√®le, API)
- **`ui.py`** : Interface utilisateur avec `rich` (panneaux, couleurs, Markdown)
- **`utils.py`** : Fonctions utilitaires (parsing de blocs code)
- **`system_prompt.txt`** : Prompt syst√®me complet (200+ lignes) d√©finissant la personnalit√© et le comportement
- **`requirements.txt`** : D√©pendances Python
- **`run.bat`** : Script de lancement rapide (Windows)

### Module LLM (`llm/`)

- **`engine.py`** : Classe `JarvisLLM`
  - G√®re la connexion Ollama
  - T√©l√©charge automatiquement les mod√®les manquants
  - Maintient l'historique conversationnel
  - G√©n√®re les r√©ponses JSON structur√©es

### Module Actions (`actions/`)

- **`__init__.py`** : Syst√®me d'enregistrement des outils
  - D√©corateur `@register_tool`
  - Registre global `TOOL_REGISTRY`
  - Fonctions `get_tools_schema()` et `execute_action()`

### Rapports (`reports/`)

Chaque conversation g√©n√®re un fichier Markdown:
- Format: `YYYY-MM-DD_HH-MM-SS_extrait_requete.md`
- Contenu: Horodatage, requ√™te, r√©ponse compl√®te

## üî¨ Fonctionnement du Syst√®me Agentique

### Pattern ReAct

J.A.R.V.I.S impl√©mente le pattern **ReAct** (Reasoning + Acting):

1. **Thought** : L'IA analyse la requ√™te et planifie
2. **Action** : Elle ex√©cute une action sp√©cifique
3. **Observation** : Le r√©sultat est inject√© dans le contexte
4. **Loop** : Retour √† l'√©tape 1 jusqu'√† obtenir une r√©ponse finale

### Structure JSON des R√©ponses

Toutes les r√©ponses du LLM suivent ce sch√©ma:

```json
{
  "thought": "Analyse interne de la situation...",
  "type": "chat" | "action",
  "content": "..." | {
    "action_name": "nom_action",
    "args": {"param": "valeur"}
  }
}
```

### Boucle Agentique (25 √âtapes Max)

```python
for step in range(25):
    response = llm.generate_action(...)
    
    if response.type == "action":
        result = execute_action(...)
        llm.add_message("user", f"OBSERVATION: {result}")
        continue  # Prochaine it√©ration
    
    elif response.type == "chat":
        display_message(response.content)
        save_report(...)
        break  # Fin de la boucle
```

### Pipeline Cognitif (Syst√®me)

Le `system_prompt.txt` d√©finit un pipeline en 10 √©tapes:

1. **Analyse de l'entr√©e** : Langue, ton, urgence
2. **Extraction d'intention** : Information, Action, Recherche, etc.
3. **Activation du contexte** : Que savons-nous d√©j√† ?
4. **Classification de t√¢che** : Simple, multi-√©tapes, risqu√© ?
5. **Raisonnement** : Comparaison des strat√©gies
6. **Planification** : √âtapes concr√®tes
7. **S√©lection d'outil** : Quel outil utiliser ?
8. **D√©cision d'ex√©cution** : Chat, Action, Clarification ?
9. **√âvaluation mentale** : Cela satisfait-il l'objectif ?
10. **D√©tection d'apprentissage** : Faut-il m√©moriser ?

## üé® Personnalisation

### Changer le Mod√®le LLM

√âditez `config.py`:

```python
MODEL_ID = "mistral:latest"        # Mod√®le plus l√©ger
MODEL_ID = "llama3.1:70b"          # Mod√®le plus puissant
MODEL_ID = "codellama:34b"         # Sp√©cialis√© code
```

Voir tous les mod√®les: `ollama list` ou [Ollama Library](https://ollama.com/library)

### Ajouter une Nouvelle Action

1. Cr√©er une fonction dans le module appropri√© (`actions/`)
2. D√©corer avec `@register_tool(description)`
3. La fonction est automatiquement disponible

**Exemple:**

```python
# Dans actions/extended.py
from . import register_tool

@register_tool("Renvoie la date et l'heure actuelles. Args: none")
def get_datetime():
    import datetime
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
```

### Personnaliser le Th√®me UI

√âditez `ui.py`, section `jarvis_theme`:

```python
jarvis_theme = Theme({
    "jarvis.header": "bold white on #005faf",  # En-t√™te JARVIS
    "jarvis.border": "#00afff",                # Bordure cyan
    "user.border": "#00ff00",                  # Bordure verte
    "action.border": "#ffd700",                # Bordure dor√©e
    # ... personnalisez les couleurs
})
```

### Modifier la Personnalit√©

√âditez `system_prompt.txt`:

```
PERSONA: "University Professor" & "High-Tech Butler"
VERBOSITY: EXTREMELY HIGH for final answers
```

Changez en:

```
PERSONA: "Casual Friend" & "Tech Expert"
VERBOSITY: MODERATE, concise but friendly
```

## üêõ D√©pannage

### Probl√®me: "Ollama not reachable"

**Solution:**
```bash
# D√©marrer Ollama manuellement
ollama serve
```

Ou v√©rifier que le service est actif:
```bash
# Windows
netstat -ano | findstr :11434

# Linux/macOS
lsof -i :11434
```

### Probl√®me: "Model not found"

**Solution:**
```bash
# Lister les mod√®les install√©s
ollama list

# T√©l√©charger le mod√®le manquant
ollama pull llama3.1:8b
```

### Probl√®me: R√©ponses en Anglais

**Solution:** V√©rifiez que `system_prompt.txt` contient:
```
LANGUAGE: STRICTLY FRENCH (Fran√ßais).
```

Et que Wikipedia est configur√© en fran√ßais (`actions/web.py`):
```python
wikipedia.set_lang("fr")
```

### Probl√®me: Erreurs JSON

**Cause:** Le mod√®le ne g√©n√®re pas toujours du JSON valide.

**Solution:**
- Utiliser un mod√®le plus grand (`llama3.1:70b`)
- Am√©liorer le prompt syst√®me
- Le syst√®me a des fallbacks pour g√©rer les r√©ponses malform√©es

### Probl√®me: Boucle Infinie d'Actions

**Solution:** Le syst√®me a une limite de 25 √©tapes. Si atteinte, il force une synth√®se finale.

### Probl√®me: D√©pendances Manquantes

```bash
# R√©installer toutes les d√©pendances
pip install -r requirements.txt --force-reinstall

# Ou installer individuellement
pip install ollama rich wikipedia google-api-python-client
```

## ü§ù Contribuer

Les contributions sont les bienvenues ! Voici comment participer:

### 1. Fork le Projet

### 2. Cr√©er une Branche

```bash
git checkout -b feature/nouvelle-action
```

### 3. Commit les Changements

```bash
git commit -m "Ajout: action de calendrier"
```

### 4. Push vers la Branche

```bash
git push origin feature/nouvelle-action
```

### 5. Ouvrir une Pull Request

### Id√©es de Contributions

- üéØ **Nouvelles actions** : Email, calendrier, notes, base de donn√©es
- üåç **Traductions** : Support multilingue
- üé® **Th√®mes UI** : Nouveaux th√®mes visuels
- üìä **Analytics** : Statistiques d'utilisation
- üß™ **Tests** : Suite de tests unitaires
- üìñ **Documentation** : Am√©liorer ce README

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

## üôè Remerciements

- [Ollama](https://ollama.com/) : Pour le runtime LLM local
- [Rich](https://github.com/Textualize/rich) : Pour l'interface terminal √©l√©gante
- [Meta AI](https://ai.meta.com/) : Pour Llama 3.1
- [Marvel/MCU](https://www.marvel.com/) : Pour l'inspiration J.A.R.V.I.S.

## üìû Contact

Pour toute question ou suggestion:
- üìß Email: votre@email.com
- üêõ Issues: [GitHub Issues](lien-vers-issues)
- üí¨ Discussions: [GitHub Discussions](lien-vers-discussions)

---

**D√©velopp√© avec ‚ù§Ô∏è pour la communaut√© open source**

*"Sometimes you gotta run before you can walk." - Tony Stark*
